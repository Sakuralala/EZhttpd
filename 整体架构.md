# 整体架构  
本项目的整体架构大致可以分为四个部分，如几个文件夹所示:bases、event、log、net，下面一一进行介绍。  
## 基础部分  
bases中的文件都是一些底层的封装:  
### 线程相关
本项目未采用C++11给出的新的thread库，主要是为了自己动手封装一下相关的底层内容以加深理解; 这部分内容都是参考的muduo的相关内容。  
#### 线程同步  
和线程通信同步相关的内容有:mutex(互斥锁)、condition(条件变量)、countdownLatch(倒计时)。这几个东西已经可以应付绝大多数场景了。  
##### mutex
和mutex相关的有两个类:Mutex和MutexGuard。Mutex的类封装的是互斥锁的init和destroy，即在构造时进行初始化，析构时自动进行destroy，它一般作为其他类的类成员;MutexGuard封装的是加解锁操作，创建一个MutesGuard实例时就会尝试lock，而当MutexGuard对象析构时就会进行unlock操作，即MutexGuard对象不能是个堆上对象，一般在临界区前面加一个大括号表示临界区，并创建一个MutexGuard对象,这样就可以轻松实现自动加解锁操作了,从而减少了错误的可能性。  
另外，上面的封装方式即所谓的RAII模式。  
##### condition  
和条件变量相关的类是Condition,其封装方式和Mutex基本类似，构造时进行init，析构时进行destroy;另外也对"等待条件"和"唤醒等待在条件上的进程"这两个操作进行了封装，其中前者有可以分为永久等待和定时等待，后者可以分为唤醒一个进程和唤醒全部进程。  
另外注意，Condition和MutexGuard中的Mutex成员均为引用,不然的话还是需要通过某种方式暴露底层的Mutex。    
##### countdownLatch 
倒计时，其实就是条件变量的简单分装而已。目的正如muduo的书中所述，用于主线程等待所有子线程初始化完成，或者用于子线程在开始工作前等待主线程完成一些工作。在主线程启动子线程的过程中用到了这个东西,子线程在进入thread function后，会先缓存一下当前线程id，然后减小倒计时从而使得条件得到满足并唤醒主线程。      
另外注意等待条件满足而睡眠时用的是while，目的是为了防止虚假唤醒。  
#### 线程及线程池  
和线程相关的类是Thread,主要封装了线程的启动和终止,在创建实例时需要提供一个子线程的thread function(在项目中由线程池进行提供),调用run会尝试启动一个子线程并缓存其pid以便在日志中标示，而join则负责等待子线程停止，在Thread的析构函数中会自动调用join以等待停止。  
另外，理一下几个thread function的层次:  
Thread::threadFuncWrapper->ThreadPool::threadFunction->实际的task。  
和线程池相关的类是ThreadPool,其内部通过std::vector来保存Thread对象，其中Thread对象又是通过std::unique_ptr包裹的，理由有两个:Thread类不支持拷贝语义，也不支持移动语义，而std::vector需要其保存的元素类型至少是moveable的；使用std::unique_ptr也符合RAII语义，也可以减少在头文件中暴露内部类，及降低编译依赖；  
这里的线程池其实是一个典型的单生产者-多消费者模型，主线程扮演生产者负责产生任务，子线程扮演消费者负责处理任务。  
线程池还有一个重要的成员，即任务队列，使用std::deque来进行实现，原因是子线程需要从头部取任务，而主线程需要从尾部添加任务。其保存的任务类型为std::function<void ()>。  
为了保证任务队列的同步性，使用了两个条件变量:notFull和notEmpty，分别表示当前任务队列是否已满，是否已空。若生产者的速度远大于消费者，那么任务队列中的任务会急剧增加，此处设置了一个最大阈值，达到这个数之后生产者(主线程)需要在notFull条件变量上进行等待，直至消费者(子线程)通知;若消费者速度远大于生产者，则当任务队列为空时需要在notEmpty条件变量上进行等待，直至新的任务到达生产者进行唤醒，注意此处唤醒只会唤醒一个。  
另外，线程池的停止分为两种方式:立刻停止和平滑停止。后者会在处理完所有任务队列中的剩余任务后才停止。这里罕见地使用了goto语句，因为需要在判断任务队列是否为空前加锁，而在处理完一个任务后需要解锁操作，这个是通过析构函数自动完成的，用while循环的话不好写。  
最后注意，本项目中并未使用bases中的线程池，因为这里的线程属于计算线程；而本项目需要的是另一种线程，即io线程，专门负责监控io,故在event中重新封装了另一种线程。可以总结一下，线程根据目的可以分为一下几种:  
1)、计算线程，上述的线程即为计算线程，常用于一些计算任务较大，需要长时间占用cpu的任务；  
2)、IO线程，主要是负责网络io操作，大部分时间等待在io上；  
3)、其他线程，如日志模块使用的后端日志线程；  
##### 线程局部变量  
本项目还使用了两种线程局部变量:  
1)、threadID，作用就是缓存当前线程的ID,避免每次需要查询当前线程ID时都需要调用syscall(SYS_gettid)系统调用；  
2)、currentThreadLoop,这个表示的是当前IO线程所拥有的EventLoop,即事件循环对象，一个IO线程只能拥有一个EventLoop且在第一次创建EventLoop时当前线程的currentThreadLoop变量就被赋值为它，后续尝试在该IO线程下再创建新的EventLoop是不允许的。  
### 日志相关  
bases中的剩余文件部分基本都和日志模块相关。  
#### 缓冲区  
由于日志模块采用了流式输出的风格,需要一个缓冲区暂存前端线程生成的一条日志语句,即将重载的<<操作符连接的几个子语句保存起来，并在适当的时机传递给后端线程对象所拥有的缓冲区。为什么不直接传递给后端线程的缓冲区呢，主要是为了对日志语句进行一下格式化处理，不然的话一条一条子语句传递，由于存在多个前端线程，子语句间会交叉写入导致日志的可读性变差。  
与缓冲区相关的类是FixedLengthBuffer,内部成员很简单，就是一个固定长度的char数组，它的最主要的操作append内部就通过调用memcpy来复制日志语句到数组中。  
#### 写文件操作 
后端日志对象需要定期将其缓冲区(队列)中的日志消息写入文件，与写入文件操作相关的类是AppendFile,它也有个缓冲区成员，原因:由于前端线程生成的日志消息比较小，每来几条消息就写入一次文件的话会导致频繁的write系统调用，故通过给中间再加一层缓冲区(64mb)收集消息，从而减少系统调用次数。另外也提供了flush操作用于强制写入内核高速缓存。    
另外，理一下一条日志消息前后经过的各个缓冲区:  
LogStream::buffer_->AsyncLog::缓存区队列中的某个块->AppendFile::buffer_->内核缓冲区高速缓存->磁盘；  
### http相关  
bases中和http相关的只有CircularBuffer类,它的作用是给请求或响应提供一个用户态的缓冲区，这是完全必要的，由于tcp协议是一个字节流的无边界的协议，读取请求的时候一次读到的消息可能是不完整的，那么需要一个用户态缓冲区保存之以便后续拼成完整的消息；发送响应的时候内核发送缓冲区可能已经满了，那么未发送完的请求需要先保存起来，以便下次写事件就绪时能发送，而且后续生成的响应也需要一个缓冲区进行保存。  
CircularBuffer是一个循环缓冲区，可能可以节省一些内存，但是在编码上的难度就要高一些，因为事实上这块缓冲区的头尾是不相交的，不管读写，遇到横跨头尾的消息时都需要额外的操作从而在外部表现地它好像是相交的一样。  
CircularBuffer内部的主要成员是vector，但是基本不使用vector提供的函数，仅仅使用它内部指向的那块内存。CircularBuffer支持满时自动扩容操作，这个操作也很直白，创建一个新的两倍大小的vector，把数据复制过去，交换指针，完事。  
CircularBuffer还有两个成员，即当前读写索引:readIndex_、writeIndex_,分别表示当前读操作的起始点，当前写操作的起始点，当CircularBuffer为空时两者会置为-1。  
CircularBuffer和http请求响应的读写强相关。其中recv函数负责将请求从内核缓冲区读取出来，由于本项目的epoll默认工作在et模式下，故需要循环读，并根据read的返回值来决定下一步,且需要根据当前读写索引的前后情况决定读到哪部分;写的话就更为复杂一些，因为写还分为两种:写事件回调的写,以及用户主动写。每次用户主动写之前需要写调用写事件回调尝试发送上一次的剩余部分，然后才是这次用户写入的部分，内部的操作其实和读类似，循环写，且需要根据读写索引的前后关系决定写到哪一部分,并且还需要把这次没写完的保存起来。  
另外，其他一些操作函数和http相关类耦合度较高，因为循环缓冲区的特殊性，这也是后续需要进行改进的地方。  

## 事件相关  
网络IO事件的处理需要一个模式来进行支撑，而此模式内部也需要一个高效的监控模型提供支持。这里所说的模式即服务端编程模型中最常使用的reactor模式，而监控模型毫无疑问，非epoll莫属。  
所谓reactor模式，程序的基本结构就是一个循环，循环内部通过事件驱动，并通过预先设置好的事件回调来进行事件的处理。这也就引出了它的两个条件:非阻塞和IO多路复用机制。这里的非阻塞指的是事件回调，即事件的处理过程不能有可能导致阻塞的部分，否则会导致整个线程被阻塞使得其他事件得不到处理；而IO多路复用，这里的复用指的是复用线程，即将当前线程等待某些事件就绪的时间用来处理已经就绪的事件,而不是串行地一个个进行处理,一个事件完全处理完再处理另一个,而多路指的是可以同时监控多个套接字描述符是否就绪，这个可以通过epoll进行实现。  
本项目最终采取的方案是main reactor+multi sub reactors的模式，即muduo中所述的方案9，和Netty中的方案相同。其中main reactor负责在对应的端口上监听新连接事件，并在新连接建立后将其分配到一个sub reactor中,后续的事件都由该sub reactor进行处理,这样也不会存在线程间可能的race condition。  
### IO线程 
如前所述，IO线程主要负责监控网络IO事件并调用相应的回调进行事件的处理。相关的类是IOThread,继承自Thread类，相比于Thread类多了一个EventLoop成员，即事件循环对象。这也是整个reactor模式的核心处理部分。  
### IO线程池 
IO线程池未采用继承bases中线程池的方式，而是重写了一个，主要成员是两个，一个就是线程集合，另一个就是与之相对应的EventLoop集合，它们的索引一一对应，这在启动线程池时设定。  
另外，再理一下thread function的调用层次:  
Thread::threadFuncWrapper->IOThread::IOThreadFunc->EventLoop::loop;  
### 事件循环  
reactor模式的核心部分就是事件循环，相关的类即EventLoop。它的主要成员有三个:  
1)、poller_，即epoll的封装，负责事件的监控；  
2)、readyList_，就绪事件的集合，poller_传回就绪的事件列表之后，会一一处理各个就绪事件；  
3)、callbackList_,其他线程交付的任务，由于reactor之间基本不进行通信，需要跨线程调配任务时需要通过runInLoop函数将任务先放到任务列表中(这部分有锁)，main reactor向sub reactor分配新连接是也是采取的这种操作；  
另外，EventLoop还有两个成员和网络IO事件不那么相关的:  
1)、wakeupEvent_,当前线程唤醒事件，也会加入到poller_成员中进行监控，以便及时唤醒当前线程处理callbackList_中的任务，这里使用了eventfd这一新的系统调用，作用是返回一个可以在epoll中进行监控的fd，因为正常的文件描述符是无法放入epoll中进行监控的，因为任何时刻它都是就绪状态；  
2)、timerSet_，定时器集合，定时器事件是直接实现在用户态的，和nginx类似，但是没做gettimeofday的缓存，因为在内核2.6.x之后gettimeofday已经是一个虚拟系统调用了，开销和普通的函数调用差不多，没必要进行缓存；  
EventLoop的处理的起点就是loop函数，其内部如前所述是一个循环,处理的流程如下:  
1)、调用poller_的poll获取就绪的事件(这也是没有就绪事件时等待的地方)；
2)、依次调用每个就绪事件的Event::handleEvent来处理事件；  
3)、处理超时的定时器事件；  
4)、处理其他线程(主要是主线程)分配的任务(主要是新连接任务)；  

### 事件  
与事件相关的类是Event，它是事件相关的封装，包含了一个事件的一切信息，包括事件的读、写、错误、关闭回调，当前事件对应的文件描述符，当前事件中实际感兴趣的事件类型等。  
注意Event并不拥有文件描述符，故在析构中不需要关闭之。  
Event的核心操作分为两种:  
1)、在epoll中注册、修改、删除当前事件,除了第一次主线程分配新连接事件给子线程时需要调用runInLoop以避免线程间的race condition之外，后续的事件处理都是由单线程负责的，故不存在race condition，那么这些操作也就当做单线程来写就行,主要通过updateEvent和remove来完成上述操作；  
2)、调用相应的事件回调处理事件，通过handleEvent函数完成，注意此处涉及到一个对象生命周期管理的问题，即Event对象或者其owner对象在回调中已经被销毁了，那么再返回这个函数调用继续后续的回调就有可能产生问题(segment fault),就像读事件回调中经过一系列调用销毁了Connection对象，而此时再回来调用写事件回调就有可能出问题，因为这里需要访问已经被释放了的缓冲区，之前测试时偶尔出现的段错误也就是这个引起的。故这里仿造muduo增加了一个tie函数，目的就是延长Event对象或者其owner对象(Connection)的生命期，使得原有的析构操作再晚一些，在handleEvent之后才析构Connection对象;另外，还要注意，目前为止除了网络事件外，监听事件、wakeup事件是不需要tie的，因为没有owner对象，不存在生命周期的问题。   
### epoller
epoll相关的封装在Epoller类中，内部有两个成员:  
1)、epollFd_,即当前epoll对象对应的文件描述符；  
2)、events_，作为传递给epoll_wait的就绪事件数组，在epoll_wait的返回值等于当前数组大小时会自动扩容；  
Epoller的核心操作分为两个:  
1)、updateEvent,在epoll对象中增删改对应的事件；  
2)、poll,内部调用epoll_wait监控已注册的事件，并把就绪的事件传回给上层EventLoop; 注意，就算没有事件就绪，也是1s醒1次；   
### timer
定时器事件的相关类在Timer及TimerSet中；关于这部分，最核心的问题就是用哪个数据结构来保存定时器节点，需要满足的条件有:  
1)、快速找到所有到期的定时器节点；  
2)、快速添加和删除定时器节点；  
上述两点都需要建立在定时器节点集合的有序性的基础上，使用线性结构如vector的话，1)通过二分查找可以降到O(logn),但是2)并不是O(logn)的，因为找到后需要进行整体的节点移动,这部分操作是O(n)的；改为使用链表的话，无法使用二分查找，那么所有的操作都是O(n)的；  
综上，剩下的可以考虑的就是二叉树、二叉堆，但是C++中的priority_queue是没有随机删除元素这个操作的，那么最终采用的就是C++中的map,底层以红黑树实现，可以满足上市两点需求。为什么不用set呢，因为set只能加，不能主动删，而使用map的话就可以通过key(实现中使用了Timer的地址)来进行快速的删除操作；  
总结一下，对于子线程而言:  
一个新连接到来时的调用流程:  
Thread::threadFuncWrapper->IOThread::IOThreadFunc->EventLoop::loop->执行callbackList_中的回调，在主线程中，它被指定为EventLoop::enableAll->Event::update->EventLoop::updateEvent->Epoller::updateEvent->epoll_ctl;  
至此之后，新连接就归某个子线程处理了，后续的事件处理流程:  
EventLoop::loop->Event::handleEvent->具体的事件回调；  

对于主线程而言:  
主线程只需要处理读事件，其调用流程:  
EventLoop::loop->Event::handleEvent->Event::readCallback_;  
至此，一个完整的reactor模式已经形成；  

## 日志模块  
日志模块，按照功能分，也可以分为三部分；  
### 日志线程
和日志线程(后端线程)相关的类就AsyncLog,它的核心成员在于两个缓冲区列表empty_、full_和当前缓冲区current_,采用了所谓的双缓存区设计(其实是进一步使用了缓冲区列表):  
1)、可用缓冲区列表empty_,存的是当下可用的缓冲区，每次前端线程写满了current_缓冲区会尝试从此列表尾部拿一块可用的，并将current_缓冲区放到full_列表的末尾部分，若无可用的则不做任何操作直接返回；  
2)、已写满缓冲区列表full_,放的是已经写满的缓冲区，供后端线程写入文件，full_中没有缓冲区时日志线程会在条件上等待几秒时间，再将current_缓冲区也挂到full_后面进行写入；  

核心操作分两个:  
1)、前端线程(非日志线程)写日志消息，提供一个统一的append函数接口，临界区是整个函数，但是其实并不耗时，因为出去一个复制日志消息到缓冲区的开销，剩下的操作都通过移动赋值来进行开销的避免了；  
2)、后端线程(日志线程)把日志消息写入文件(其实也存在一个缓冲区，目的是减小系统调用次数),函数为writeToFile,这个其实是线程的thread function,这里的临界区分为两部分，第一部分是获取full_列表，这里使用了和EventLoop中callbackList_一样的技巧，即通过vector的swap操作来减小临界区的耗时，然后就是写文件操作；第二部分是为了把已写入的缓冲区全部挂回到empty_中；  

### 写入文件操作  
后端写成写入文件的相关操作也做了一个封装，如果只是写入文件操作的话没必要做这层封装，bases中的AppendFile已经够用了，这里封装的目的是为了支持自动创建新的日志文件并在旧的日志文件超过一定大小时进行归档操作(创建新的日志文件)。  
主要成员就是AppendFile,没啥好说的；  
主要操作分两个:  
1)、向文件写，append函数，内部其实主要就是调用AppendFile::append,并根据当前文件的大小来rollFile;  
2)、日志文件的创建和归档都在rollFile函数中，创建新文件其实不是啥问题，关键在于命名，此处采取的方式是使用当前时间(格式化成年月日时分秒微妙的形式)，精确到微秒;  

### 流式风格 
光只提供一个AsyncLog::append接口无疑是很不方便的，为了像cout那样流式输出，额外提供了两个类:LogStream和Logger。
#### LogStream
LogStream类内部存在一个小规模的buffer用于保存<<操作符的各个子消息，其核心部分在于重载了接收常见的各个类型的<<操作符,包括各种类型的数字、char、char*、string、bool。  
#### Logger
有了LogStream之后其实就可以通过宏定义:  
```cpp
#define LOG LogStream()
```
来实现类似:  
```cpp
LOG<<xxx<<aaa;
```
的操作了，Logger存在的意义就在于提供一个日志输出等级，以及格式化输出的日志消息，在创建Logger对象时会在日志消息前面加上固定的前缀，包括:时间、当前线程id、log级别；而在析构Logger对象时会在最后加上对应的源文件及行号，并通过单例模式来调用AsyncLog::append函数,具体的单例模式是Async版本的特化，通过pthread_once来实现日志线程的启动仅进行一次，而单例模式的多线程安全直接通过C++11要求的局部静态成员的多线程安全性来进行保证(C++11要求编译器保证局部静态成员的线程安全性)。  





